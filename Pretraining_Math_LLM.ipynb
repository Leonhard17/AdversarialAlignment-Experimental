{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "175d59e3",
   "metadata": {},
   "source": [
    "# Pretrain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b849b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "\n",
    "# Define the MathDataset class\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        problem = self.dataframe.iloc[idx][\"problem\"]\n",
    "        solution = self.dataframe.iloc[idx][\"solution\"]\n",
    "\n",
    "        return problem, solution\n",
    "\n",
    "# collate_fn function to handle padding and tokenization for a whole batch\n",
    "def collate_fn(batch):\n",
    "    problems, solutions = zip(*batch)\n",
    "    split_token = \" =\" # has additional space in front as this is a special token\n",
    "    split_token_id = tokenizer.encode(split_token)[0]\n",
    "\n",
    "    questions = [f\"{p} {s}{tokenizer.eos_token}\" for p, s in zip(problems, solutions)] # concatenate and add eos_token\n",
    "\n",
    "    encoder = tokenizer(\n",
    "        questions,  # Concatenate problems and solutions for encoding\n",
    "        padding=True,\n",
    "        padding_side=\"left\",\n",
    "        truncation=True,\n",
    "        max_length=20, # TODO: Adjust max_length based on model\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # mask the labels for the solutions\n",
    "    labels = encoder[\"input_ids\"].clone()\n",
    "    for i in range(len(problems)):\n",
    "        # Find the index of the split token in the input_ids\n",
    "        split_index = (encoder[\"input_ids\"][i] == split_token_id).nonzero(as_tuple=True)[0]\n",
    "        # Set the labels to -100 for the problem part, so they won't be used in loss calculation\n",
    "        labels[i][:(split_index+1)] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": encoder[\"input_ids\"],\n",
    "        \"attention_mask\": encoder[\"attention_mask\"],\n",
    "        \"labels\": labels,  # Use the masked labels for loss calculation\n",
    "    }\n",
    "\n",
    "\n",
    "# Load the math dataset\n",
    "def load_math_data(problem_filename=\"math_problems.txt\", solution_filename=\"math_solutions.txt\"):\n",
    "    import pandas as pd\n",
    "    problems = [line.strip() for line in open(problem_filename, \"r\")]\n",
    "    solutions = [line.strip() for line in open(solution_filename, \"r\")]\n",
    "    return pd.DataFrame({\"problem\": problems, \"solution\": solutions})\n",
    "\n",
    "data = load_math_data(\"math_problems.txt\", \"math_solutions.txt\")\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "train_data = data[:train_size]\n",
    "test_data = data[train_size:]\n",
    "\n",
    "# Initialize tokenizer and dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})  # Explicitly add a special padding token\n",
    "    tokenizer.pad_token = '<|pad|>'\n",
    "\n",
    "train_math_dataset = MathDataset(train_data)\n",
    "test_math_dataset = MathDataset(test_data)\n",
    "\n",
    "train_data_loader = DataLoader(train_math_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(test_math_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae40a36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: ['30 * 51 = 1530<|endoftext|>', '<|pad|>49 + 11 = 60<|endoftext|>', '2726 / 94 = 29<|endoftext|>', '<|pad|>73 * 4 = 292<|endoftext|>', '<|pad|>66 + 35 = 101<|endoftext|>', '<|pad|>65 + 76 = 141<|endoftext|>', '2132 / 52 = 41<|endoftext|>', '93 * 44 = 4092<|endoftext|>', '8 - 89 = -81<|endoftext|>', '21 - 93 = -72<|endoftext|>', '<|pad|>11 * 13 = 143<|endoftext|>', '22 * 28 = 616<|endoftext|>', '<|pad|>82 - 21 = 61<|endoftext|>', '<|pad|>3 + 79 = 82<|endoftext|>', '<|pad|>18 + 41 = 59<|endoftext|>', '<|pad|>59 - 52 = 7<|endoftext|>', '<|pad|>100 - 93 = 7<|endoftext|>', '<|pad|>38 / 1 = 38<|endoftext|>', '<|pad|>94 - 64 = 30<|endoftext|>', '79 * 46 = 3634<|endoftext|>', '1782 / 99 = 18<|endoftext|>', '2240 / 80 = 28<|endoftext|>', '<|pad|>93 + 27 = 120<|endoftext|>', '<|pad|>7 * 72 = 504<|endoftext|>', '45 - 68 = -23<|endoftext|>', '13 - 45 = -32<|endoftext|>', '<|pad|>83 + 17 = 100<|endoftext|>', '62 * 16 = 992<|endoftext|>', '<|pad|>29 + 32 = 61<|endoftext|>', '3726 / 46 = 81<|endoftext|>', '<|pad|>55 * 4 = 220<|endoftext|>', '88 * 87 = 7656<|endoftext|>']\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1]])\n",
      "Labels: tensor([[ -100,  -100,  -100,  -100,  1315,  1270, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  3126, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  2808, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100, 41569, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  8949, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100, 25500, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  6073, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  2319,  5892, 50256],\n",
      "        [ -100,  -100,  -100,  -100,   532,  6659, 50256],\n",
      "        [ -100,  -100,  -100,  -100,   532,  4761, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100, 24356, 50256],\n",
      "        [ -100,  -100,  -100,  -100,   718,  1433, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  8454, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  9415, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  7863, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100,   767, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100,   767, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  4353, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  1542, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  4570,  2682, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  1248, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  2579, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  7982, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100, 41612, 50256],\n",
      "        [ -100,  -100,  -100,  -100,   532,  1954, 50256],\n",
      "        [ -100,  -100,  -100,  -100,   532,  2624, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  1802, 50256],\n",
      "        [ -100,  -100,  -100,  -100,   860,  5892, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  8454, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  9773, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100, 15629, 50256],\n",
      "        [ -100,  -100,  -100,  -100,   767, 37466, 50256]])\n"
     ]
    }
   ],
   "source": [
    "# Used to test the Dataloader\n",
    "# Extract a single batch from the DataLoader\n",
    "batch = next(iter(train_data_loader))\n",
    "\n",
    "# Access the batch data\n",
    "input_ids = batch[\"input_ids\"]\n",
    "attention_mask = batch[\"attention_mask\"]\n",
    "labels = batch[\"labels\"]\n",
    "\n",
    "# Decode and print the batch data\n",
    "print(\"Input IDs:\", tokenizer.batch_decode(input_ids.tolist(), skip_special_tokens=False))\n",
    "print(\"Attention Mask:\", attention_mask)\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4ba06b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "c:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Progress: 100%|██████████| 1000/1000 [01:13<00:00, 13.64it/s, loss=1.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Validation Loss: 1.5242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training Progress: 100%|██████████| 1000/1000 [01:12<00:00, 13.76it/s, loss=1.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4, Validation Loss: 1.1724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training Progress: 100%|██████████| 1000/1000 [01:11<00:00, 14.08it/s, loss=1.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4, Validation Loss: 1.0280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training Progress: 100%|██████████| 1000/1000 [01:11<00:00, 14.02it/s, loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4, Validation Loss: 0.9710\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 4\n",
    "\n",
    "# Load the GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define optimizer and training parameters\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_training_steps = num_epochs * len(train_data_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    progress_bar = tqdm(train_data_loader, desc=f\"Epoch {epoch+1} Training Progress\") # Add progress bar\n",
    "    model.train()\n",
    "    for batch in progress_bar:\n",
    "        # Move batch to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # validation loss calculation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_samples = 0\n",
    "    for batch in test_data_loader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss = outputs.loss\n",
    "\n",
    "            batch_size = input_ids.size(0)\n",
    "            total_val_loss += val_loss.item() * batch_size\n",
    "            total_val_samples += batch_size\n",
    "    # Calculate average validation loss\n",
    "    mean_val_loss = total_val_loss / total_val_samples if total_val_samples > 0 else 0\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {mean_val_loss:.4f}\")\n",
    "    # Save the finetuned model\n",
    "    model.save_pretrained(\"finetuned_gpt2_math_epoch_{}\".format(epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e93537e",
   "metadata": {},
   "source": [
    "# Load the Finetuned Model and Run Inference\n",
    "This section demonstrates how to load the finetuned GPT-2 model and use it to generate solutions for new math problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "404c178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load the finetuned model and tokenizer\n",
    "model_path = \"finetuned_gpt2_math_epoch_4\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Ensure the model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to generate a solution for a given math problem\n",
    "def generate_solution(problem, max_length=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the input problem\n",
    "        input_enc = tokenizer(\n",
    "            problem,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        input_ids = input_enc[\"input_ids\"].to(device)\n",
    "        attention_mask = input_enc[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Generate output\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        # Decode the generated output\n",
    "        solution = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b912e555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem: 89 * 61 =\n",
      "Solution: 89 * 61 = 5551\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Select a random example from the test set for inference\n",
    "random_idx = random.randint(0, len(test_data) - 1)\n",
    "example_problem = test_data.iloc[random_idx][\"problem\"]\n",
    "solution = generate_solution(example_problem)\n",
    "print(f\"Problem: {example_problem}\")\n",
    "print(f\"Solution: {solution}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adverserialAlignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
