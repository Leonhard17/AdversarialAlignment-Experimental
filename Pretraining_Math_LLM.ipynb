{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "175d59e3",
   "metadata": {},
   "source": [
    "# Pretrain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b849b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "\n",
    "# Define the MathDataset class\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        problem = self.dataframe.iloc[idx][\"problem\"]\n",
    "        solution = self.dataframe.iloc[idx][\"solution\"]\n",
    "\n",
    "        return problem, solution\n",
    "\n",
    "# collate_fn function to handle padding and tokenization for a whole batch\n",
    "def collate_fn(batch):\n",
    "    problems, solutions = zip(*batch)\n",
    "    split_token = \" =\" # has additional space in front as this is a special token\n",
    "    split_token_id = tokenizer.encode(split_token)[0]\n",
    "\n",
    "    questions = [f\"{p} {s}{tokenizer.eos_token}\" for p, s in zip(problems, solutions)] # concatenate and add eos_token\n",
    "\n",
    "    encoder = tokenizer(\n",
    "        questions,  # Concatenate problems and solutions for encoding\n",
    "        padding=True,\n",
    "        padding_side=\"left\",\n",
    "        truncation=True,\n",
    "        max_length=20, # TODO: Adjust max_length based on model\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # mask the labels for the solutions\n",
    "    labels = encoder[\"input_ids\"].clone()\n",
    "    for i in range(len(problems)):\n",
    "        # Find the index of the split token in the input_ids\n",
    "        split_index = (encoder[\"input_ids\"][i] == split_token_id).nonzero(as_tuple=True)[0]\n",
    "        # Set the labels to -100 for the problem part, so they won't be used in loss calculation\n",
    "        labels[i][:(split_index+1)] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": encoder[\"input_ids\"],\n",
    "        \"attention_mask\": encoder[\"attention_mask\"],\n",
    "        \"labels\": labels,  # Use the masked labels for loss calculation\n",
    "    }\n",
    "\n",
    "\n",
    "# Load the math dataset\n",
    "def load_math_data(problem_filename=\"math_problems.txt\", solution_filename=\"math_solutions.txt\"):\n",
    "    import pandas as pd\n",
    "    problems = [line.strip() for line in open(problem_filename, \"r\")]\n",
    "    solutions = [line.strip() for line in open(solution_filename, \"r\")]\n",
    "    return pd.DataFrame({\"problem\": problems, \"solution\": solutions})\n",
    "\n",
    "data = load_math_data(\"math_problems.txt\", \"math_solutions.txt\")\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "train_data = data[:train_size]\n",
    "test_data = data[train_size:]\n",
    "\n",
    "# Initialize tokenizer and dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})  # Explicitly add a special padding token\n",
    "    tokenizer.pad_token = '<|pad|>'\n",
    "\n",
    "train_math_dataset = MathDataset(train_data)\n",
    "test_math_dataset = MathDataset(test_data)\n",
    "\n",
    "train_data_loader = DataLoader(train_math_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(test_math_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae40a36b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Used to test the Dataloader\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Extract a single batch from the DataLoader\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mdata_loader\u001b[49m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Access the batch data\u001b[39;00m\n\u001b[0;32m      7\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Used to test the Dataloader\n",
    "# Extract a single batch from the DataLoader\n",
    "batch = next(iter(train_data_loader))\n",
    "\n",
    "# Access the batch data\n",
    "input_ids = batch[\"input_ids\"]\n",
    "attention_mask = batch[\"attention_mask\"]\n",
    "labels = batch[\"labels\"]\n",
    "\n",
    "# Decode and print the batch data\n",
    "print(\"Input IDs:\", tokenizer.batch_decode(input_ids.tolist(), skip_special_tokens=False))\n",
    "print(\"Attention Mask:\", attention_mask)\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4ba06b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Progress: 100%|██████████| 1000/1000 [01:11<00:00, 14.04it/s, loss=1.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Validation Loss: 1.5739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training Progress: 100%|██████████| 1000/1000 [01:10<00:00, 14.14it/s, loss=1.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4, Validation Loss: 1.2184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training Progress: 100%|██████████| 1000/1000 [01:10<00:00, 14.10it/s, loss=1.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4, Validation Loss: 1.0605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training Progress: 100%|██████████| 1000/1000 [01:10<00:00, 14.17it/s, loss=1.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4, Validation Loss: 0.9926\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 4\n",
    "\n",
    "# Load the GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define optimizer and training parameters\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_training_steps = num_epochs * len(train_data_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    progress_bar = tqdm(train_data_loader, desc=f\"Epoch {epoch+1} Training Progress\") # Add progress bar\n",
    "    model.train()\n",
    "    for batch in progress_bar:\n",
    "        # Move batch to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # validation loss calculation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_samples = 0\n",
    "    for batch in test_data_loader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss = outputs.loss\n",
    "\n",
    "            batch_size = input_ids.size(0)\n",
    "            total_val_loss += val_loss.item() * batch_size\n",
    "            total_val_samples += batch_size\n",
    "    # Calculate average validation loss\n",
    "    mean_val_loss = total_val_loss / total_val_samples if total_val_samples > 0 else 0\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {mean_val_loss:.4f}\")\n",
    "    # Save the finetuned model\n",
    "    model.save_pretrained(\"finetuned_gpt2_math_epoch_{}\".format(epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e93537e",
   "metadata": {},
   "source": [
    "# Load the Finetuned Model and Run Inference\n",
    "This section demonstrates how to load the finetuned GPT-2 model and use it to generate solutions for new math problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem: 35 * 28 =\n",
      "Solution: 35 * 28 = 940\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load the finetuned model and tokenizer\n",
    "model_path = \"finetuned_gpt2_math_epoch_4\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Ensure the model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to generate a solution for a given math problem\n",
    "def generate_solution(problem, max_length=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the input problem\n",
    "        input_enc = tokenizer(\n",
    "            problem,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        input_ids = input_enc[\"input_ids\"].to(device)\n",
    "        attention_mask = input_enc[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Generate output\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        # Decode the generated output\n",
    "        solution = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b912e555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem: 24 * 22 =\n",
      "Solution: 24 * 22 = 576\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Select a random example from the test set for inference\n",
    "random_idx = random.randint(0, len(test_data) - 1)\n",
    "example_problem = test_data.iloc[random_idx][\"problem\"]\n",
    "solution = generate_solution(example_problem)\n",
    "print(f\"Problem: {example_problem}\")\n",
    "print(f\"Solution: {solution}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adverserialAlignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
