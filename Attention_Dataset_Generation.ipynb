{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d7d07a",
   "metadata": {},
   "source": [
    "Import primary model as Datagenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e0cbc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "\n",
    "# Define the MathDataset class\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        problem = self.dataframe.iloc[idx][\"problem\"]\n",
    "        solution = self.dataframe.iloc[idx][\"solution\"]\n",
    "\n",
    "        return problem, solution\n",
    "\n",
    "# collate_fn function to handle padding and tokenization for a whole batch\n",
    "def collate_fn(batch):\n",
    "    problems, solutions = zip(*batch)\n",
    "    split_token = \" =\" # has additional space in front as this is a special token\n",
    "    split_token_id = tokenizer.encode(split_token)[0]\n",
    "\n",
    "    questions = [f\"{p} {s}{tokenizer.eos_token}\" for p, s in zip(problems, solutions)] # concatenate and add eos_token\n",
    "\n",
    "    encoder = tokenizer(\n",
    "        questions,  # Concatenate problems and solutions for encoding\n",
    "        padding=True,\n",
    "        padding_side=\"left\",\n",
    "        truncation=True,\n",
    "        max_length=20, # TODO: Adjust max_length based on model\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # mask the labels for the solutions\n",
    "    labels = encoder[\"input_ids\"].clone()\n",
    "    for i in range(len(problems)):\n",
    "        # Find the index of the split token in the input_ids\n",
    "        split_index = (encoder[\"input_ids\"][i] == split_token_id).nonzero(as_tuple=True)[0]\n",
    "        # Set the labels to -100 for the problem part, so they won't be used in loss calculation\n",
    "        labels[i][:(split_index+1)] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": encoder[\"input_ids\"],\n",
    "        \"attention_mask\": encoder[\"attention_mask\"],\n",
    "        \"labels\": labels,  # Use the masked labels for loss calculation\n",
    "    }\n",
    "\n",
    "\n",
    "# Load the math dataset\n",
    "def load_math_data(problem_filename=\"math_problems.txt\", solution_filename=\"math_solutions.txt\"):\n",
    "    import pandas as pd\n",
    "    problems = [line.strip() for line in open(problem_filename, \"r\")]\n",
    "    solutions = [line.strip() for line in open(solution_filename, \"r\")]\n",
    "    return pd.DataFrame({\"problem\": problems, \"solution\": solutions})\n",
    "\n",
    "data = load_math_data(\"math_problems.txt\", \"math_solutions.txt\")\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "train_data = data[:train_size]\n",
    "test_data = data[train_size:]\n",
    "\n",
    "# Initialize tokenizer and dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})  # Explicitly add a special padding token\n",
    "    tokenizer.pad_token = '<|pad|>'\n",
    "\n",
    "train_math_dataset = MathDataset(train_data)\n",
    "test_math_dataset = MathDataset(test_data)\n",
    "\n",
    "train_data_loader = DataLoader(train_math_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(test_math_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fdafbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load the finetuned model and tokenizer\n",
    "model_path = \"finetuned_gpt2_math_epoch_4\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Ensure the model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to generate a solution for a given math problem\n",
    "def generate_solution(problem, max_length=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the input problem\n",
    "        input_enc = tokenizer(\n",
    "            problem,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        input_ids = input_enc[\"input_ids\"].to(device)\n",
    "        attention_mask = input_enc[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Generate output\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        # extract the attention weights\n",
    "        \n",
    "\n",
    "        # Decode the generated output\n",
    "        solution = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return solution\n",
    "\n",
    "def generate_with_attentions(problem, max_new_tokens=10):\n",
    "    model.eval()\n",
    "    attentions_per_step = []\n",
    "    generated_ids = []\n",
    "\n",
    "    # Tokenize the input problem\n",
    "    input_enc = tokenizer(problem, return_tensors=\"pt\")\n",
    "    input_ids = input_enc[\"input_ids\"].to(device)\n",
    "    attention_mask = input_enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Start with the input_ids as the initial sequence\n",
    "    cur_ids = input_ids\n",
    "    for step in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=cur_ids,\n",
    "                attention_mask=torch.ones_like(cur_ids),\n",
    "                output_attentions=True,\n",
    "                return_dict=True\n",
    "            )\n",
    "            # Get logits for the last token\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            # Greedy decoding: pick the most likely next token\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "            # Save the attention weights for this step\n",
    "            attentions_per_step.append([a.cpu() for a in outputs.attentions])\n",
    "            # Append the generated token\n",
    "            cur_ids = torch.cat([cur_ids, next_token_id], dim=1)\n",
    "            generated_ids.append(next_token_id.item())\n",
    "            # Stop if EOS token is generated\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    # Decode the full sequence (input + generated)\n",
    "    full_sequence = cur_ids[0].cpu().tolist()\n",
    "    solution = tokenizer.decode(full_sequence, skip_special_tokens=True)\n",
    "    return solution, attentions_per_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64874833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`GPT2SdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "example_problem = test_data.iloc[0][\"problem\"]\n",
    "solution, attentions = generate_with_attentions(example_problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2be06c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem: 49 - 9 =\n",
      "Generated Solution: 49 - 9 = -2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Problem: {example_problem}\")\n",
    "print(f\"Generated Solution: {solution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93c1fc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "print(attentions[2][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7f39f",
   "metadata": {},
   "source": [
    "Run the model and extract the attention and calculate the reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71f98cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut off the not used solution part\n",
    "def trim_solution(solution):\n",
    "    # Find the index of the split token in the solution\n",
    "    split_token = \" =\"\n",
    "    split_index = solution.find(split_token)\n",
    "    if split_index != -1:\n",
    "        # Trim the solution to only include the part after the problem\n",
    "        trimmed_solution = solution[split_index + len(split_token):].strip()\n",
    "    else:\n",
    "        trimmed_solution = solution.strip()\n",
    "    return trimmed_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfe60efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data for the dataset\n",
    "attention_data = []\n",
    "reward_data = []\n",
    "\n",
    "# generate attention reward pairs\n",
    "for i in range(100): \n",
    "    # get problem, model solution and attention weights\n",
    "    example_problem = test_data.iloc[i][\"problem\"]\n",
    "    solution, attentions = generate_with_attentions(example_problem)\n",
    "    # trim solution for reward calculation\n",
    "    solution = trim_solution(solution)\n",
    "    # get the real solution and calculate the reward\n",
    "    example_solution = test_data.iloc[i][\"solution\"]\n",
    "    diff = torch.tensor([abs(float(solution) - float(example_solution) + 1e-6)])\n",
    "    reward = -torch.log(diff).item()\n",
    "\n",
    "    # collect for dataset\n",
    "    attention_data.append(attentions)\n",
    "    reward_data.append(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6753d91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(attention_data))\n",
    "print(len(reward_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57680866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put data into a dataset for training\n",
    "# TODO for attention extract number of layer and solution lenght\n",
    "# TODO Return size with getitem\n",
    "\n",
    "class AttentionDataset:\n",
    "    def __init__(self, attentions, rewards):\n",
    "        self.attentions = attentions\n",
    "        self.rewards = rewards\n",
    "        self.dataset_size = len(rewards)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.attentions[idx], self.rewards[idx]\n",
    "    \n",
    "def attention_collate_fn(batch):\n",
    "    attentions, rewards = zip(*batch)\n",
    "    return list(attentions), torch.tensor(rewards)\n",
    "    \n",
    "test_attention_dataset = AttentionDataset(attention_data, reward_data)\n",
    "test_attention_loader = DataLoader(test_attention_dataset, batch_size=32, shuffle=False, collate_fn=attention_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5ba6fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n",
      "Rewards: tensor([-3.7377e+00, -6.8341e+00,  1.3816e+01, -6.9315e-01, -8.5150e+00,\n",
      "        -6.1159e+00,  1.3816e+01, -4.1589e+00, -3.8712e+00,  1.3816e+01,\n",
      "        -4.4427e+00, -6.1137e+00,  1.3816e+01, -1.7918e+00, -5.8051e+00,\n",
      "        -4.7875e+00, -2.9957e+00, -5.7038e+00, -4.3820e+00, -6.8330e+00,\n",
      "        -3.5264e+00, -2.3026e+00, -3.4012e+00,  1.3816e+01, -6.9315e-01,\n",
      "        -7.8766e+00,  1.3816e+01, -5.5835e+00, -3.8286e+00, -4.3694e+00,\n",
      "        -1.6094e+00,  1.0133e-06])\n",
      "Attention data len0: 32\n",
      "Attention data len1: 2\n",
      "Attention data len2: 12\n",
      "Attention shape: torch.Size([1, 12, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "for batch in test_attention_loader:\n",
    "    attentions, rewards = batch\n",
    "    # Process the batch as needed\n",
    "    print(f\"Batch size: {len(attentions)}\")\n",
    "    print(f\"Rewards: {rewards}\")\n",
    "    print(f\"Attention data len0: {len(attentions)}\")\n",
    "    print(f\"Attention data len1: {len(attentions[2])}\")\n",
    "    print(f\"Attention data len2: {len(attentions[3][0])}\")\n",
    "    print(f\"Attention shape: {attentions[2][1][0].shape}\")\n",
    "    break  # Just to check the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c59dfef",
   "metadata": {},
   "source": [
    "Saving the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8560c31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset size: 100\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# TODO: optinally change to json format to make data human readable\n",
    "# Save the dataset to a file\n",
    "with open(\"attention_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump((attention_data, reward_data), f)\n",
    "# Load the dataset from a file\n",
    "with open(\"attention_dataset.pkl\", \"rb\") as f:\n",
    "    loaded_attention_data, loaded_reward_data = pickle.load(f)\n",
    "print(f\"Loaded dataset size: {len(loaded_reward_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254b045c",
   "metadata": {},
   "source": [
    "Dataset adaption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a9d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "\n",
    "# load data using pickle\n",
    "with open(\"attention_dataset.pkl\", \"rb\") as f:\n",
    "    \"\"\"\n",
    "    attention_data: dimension (num_samples, iteration, layer, \n",
    "                                1 #batch during generation, \n",
    "                                num_heads, seq_len, seq_len)\n",
    "    reward_data: list of rewards (num_samples)\n",
    "    \"\"\"\n",
    "    attention_data, reward_data = pickle.load(f)\n",
    "\n",
    "# AttentionDataset class\n",
    "class AttentionDataset:\n",
    "    def __init__(self, attentions, rewards):\n",
    "        self.attentions = attentions\n",
    "        self.rewards = rewards\n",
    "        self.dataset_size = len(rewards)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.attentions[idx], self.rewards[idx]\n",
    "    \n",
    "def attention_collate_fn(batch):\n",
    "    attentions, rewards = zip(*batch)\n",
    "    return list(attentions), torch.tensor(rewards)\n",
    "\n",
    "attention_dataset = AttentionDataset(attention_data, reward_data)\n",
    "attention_loader = DataLoader(attention_dataset, batch_size=32, shuffle=False, collate_fn=attention_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09eb7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to convert the attention data to a graph\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Create the graph structure from the attention weights\n",
    "def attention_to_graph(attention):\n",
    "    # Get the number of nodes\n",
    "    n = attention.shape[-1] # number of tokens\n",
    "\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes from attention\n",
    "    for i in range(n):\n",
    "        # TODO: weight dependend on the number of the tokens\n",
    "        G.add_node(f'token_{i}', weight=attention[i, i])\n",
    "    \n",
    "    # Add edges from attention\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if (j < i): # attention masking\n",
    "                G.add_edge(f'token_{i}', f'token_{j}', weight=attention[i, j])\n",
    "\n",
    "    # TODO: Check for further aggregation for transformer input\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0cdefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the attention_to_graph function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Attention data shape:\", attention_data[0][0][0][0][0].shape)  # Check the shape of the attention data\n",
    "print(\"Attention data:\", len(attention_data))  # Check the attention data\n",
    "print(\"Attention data:\", len(attention_data[0]))  # Check the attention data\n",
    "test_graph = attention_to_graph(attention_data[0][0][0][0][0])\n",
    "\n",
    "def print_graph_details(G):\n",
    "    print(\"Nodes:\")\n",
    "    for node, data in G.nodes(data=True):\n",
    "        print(f\"{node}: {data}\")\n",
    "    \n",
    "    print(\"\\nEdges:\")\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        print(f\"{u} -> {v}: {data}\")\n",
    "\n",
    "print_graph_details(test_graph)\n",
    "\n",
    "# Function to plot the graph\n",
    "def plot_graph(G):\n",
    "    pos = nx.spring_layout(G)  # Layout for visualization\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    nx.draw(G, pos, with_labels=True, node_size=500, node_color=\"skyblue\", font_size=10, font_weight=\"bold\", edge_color=\"gray\")\n",
    "    plt.title('Attention Graph')\n",
    "    plt.show()\n",
    "\n",
    "plot_graph(test_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd9a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test graph creation for a whole batch\n",
    "\n",
    "attention_graph_data = []\n",
    "for batch in attention_loader:\n",
    "    # batch: (num_samples, iteration, layer, 1, num_heads, seq_len, seq_len), rewards: (num_samples)\n",
    "    attentions, rewards = batch\n",
    "    attention_data_batch = []\n",
    "    for attention_iteration in attentions[:]:\n",
    "        # attention_iteration: (iteration, layer, 1, num_heads, seq_len, seq_len)\n",
    "        attention_data_step = []\n",
    "        for attention_layer in attention_iteration[:]:\n",
    "            # attention_layer: (layer, 1, num_heads, seq_len, seq_len)\n",
    "            attention_data_head = []\n",
    "            for attention_batch in attention_layer[0]: # TODO: Change data structure to remove the singleton batch dimension\n",
    "                for attention_head in attention_batch[:]:\n",
    "                    # attention_head: (1, num_heads, seq_len, seq_len)\n",
    "                    G = attention_to_graph(attention_head)\n",
    "                    attention_data_head.append(G)\n",
    "            attention_data_step.append(attention_data_head)\n",
    "        attention_data_batch.append(attention_data_step)\n",
    "    attention_graph_data.append(attention_data_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ec050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the attention graph data\n",
    "print(\"Attention Graph Data Shape:\", len(attention_graph_data))\n",
    "print(\"Attention Graph Data:\", len(attention_graph_data[0])) # batch\n",
    "print(\"Attention Graph Data:\", len(attention_graph_data[0][0])) # step\n",
    "print(\"Attention Graph Data:\", len(attention_graph_data[0][0][0])) # layer\n",
    "print(\"Attention Graph Data:\", len(attention_graph_data[0][0][0][0]))  # head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4263142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_graph_details(attention_graph_data[0][0][0][0])  # Print details of the first graph\n",
    "plot_graph(attention_graph_data[0][0][0][0])  # Plot the first graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adverserialAlignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
