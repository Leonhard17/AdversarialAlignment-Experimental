{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d7d07a",
   "metadata": {},
   "source": [
    "Import primary model as Datagenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e0cbc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "\n",
    "# Define the MathDataset class\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        problem = self.dataframe.iloc[idx][\"problem\"]\n",
    "        solution = self.dataframe.iloc[idx][\"solution\"]\n",
    "\n",
    "        return problem, solution\n",
    "\n",
    "# collate_fn function to handle padding and tokenization for a whole batch\n",
    "def collate_fn(batch):\n",
    "    problems, solutions = zip(*batch)\n",
    "    split_token = \" =\" # has additional space in front as this is a special token\n",
    "    split_token_id = tokenizer.encode(split_token)[0]\n",
    "\n",
    "    questions = [f\"{p} {s}{tokenizer.eos_token}\" for p, s in zip(problems, solutions)] # concatenate and add eos_token\n",
    "\n",
    "    encoder = tokenizer(\n",
    "        questions,  # Concatenate problems and solutions for encoding\n",
    "        padding=True,\n",
    "        padding_side=\"left\",\n",
    "        truncation=True,\n",
    "        max_length=20, # TODO: Adjust max_length based on model\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # mask the labels for the solutions\n",
    "    labels = encoder[\"input_ids\"].clone()\n",
    "    for i in range(len(problems)):\n",
    "        # Find the index of the split token in the input_ids\n",
    "        split_index = (encoder[\"input_ids\"][i] == split_token_id).nonzero(as_tuple=True)[0]\n",
    "        # Set the labels to -100 for the problem part, so they won't be used in loss calculation\n",
    "        labels[i][:(split_index+1)] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": encoder[\"input_ids\"],\n",
    "        \"attention_mask\": encoder[\"attention_mask\"],\n",
    "        \"labels\": labels,  # Use the masked labels for loss calculation\n",
    "    }\n",
    "\n",
    "\n",
    "# Load the math dataset\n",
    "def load_math_data(problem_filename=\"math_problems.txt\", solution_filename=\"math_solutions.txt\"):\n",
    "    import pandas as pd\n",
    "    problems = [line.strip() for line in open(problem_filename, \"r\")]\n",
    "    solutions = [line.strip() for line in open(solution_filename, \"r\")]\n",
    "    return pd.DataFrame({\"problem\": problems, \"solution\": solutions})\n",
    "\n",
    "data = load_math_data(\"math_problems.txt\", \"math_solutions.txt\")\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "train_data = data[:train_size]\n",
    "test_data = data[train_size:]\n",
    "\n",
    "# Initialize tokenizer and dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})  # Explicitly add a special padding token\n",
    "    tokenizer.pad_token = '<|pad|>'\n",
    "\n",
    "train_math_dataset = MathDataset(train_data)\n",
    "test_math_dataset = MathDataset(test_data)\n",
    "\n",
    "train_data_loader = DataLoader(train_math_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(test_math_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fdafbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load the finetuned model and tokenizer\n",
    "model_path = \"finetuned_gpt2_math_epoch_4\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Ensure the model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to generate a solution for a given math problem\n",
    "def generate_solution(problem, max_length=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the input problem\n",
    "        input_enc = tokenizer(\n",
    "            problem,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        input_ids = input_enc[\"input_ids\"].to(device)\n",
    "        attention_mask = input_enc[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Generate output\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        # extract the attention weights\n",
    "        \n",
    "\n",
    "        # Decode the generated output\n",
    "        solution = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return solution\n",
    "\n",
    "def generate_with_attentions(problem, max_new_tokens=10):\n",
    "    model.eval()\n",
    "    attentions_per_step = []\n",
    "    generated_ids = []\n",
    "\n",
    "    # Tokenize the input problem\n",
    "    input_enc = tokenizer(problem, return_tensors=\"pt\")\n",
    "    input_ids = input_enc[\"input_ids\"].to(device)\n",
    "    attention_mask = input_enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Start with the input_ids as the initial sequence\n",
    "    cur_ids = input_ids\n",
    "    for step in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=cur_ids,\n",
    "                attention_mask=torch.ones_like(cur_ids),\n",
    "                output_attentions=True,\n",
    "                return_dict=True\n",
    "            )\n",
    "            # Get logits for the last token\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            # Greedy decoding: pick the most likely next token\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "            # Save the attention weights for this step\n",
    "            attentions_per_step.append([a.cpu() for a in outputs.attentions])\n",
    "            # Append the generated token\n",
    "            cur_ids = torch.cat([cur_ids, next_token_id], dim=1)\n",
    "            generated_ids.append(next_token_id.item())\n",
    "            # Stop if EOS token is generated\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    # Decode the full sequence (input + generated)\n",
    "    full_sequence = cur_ids[0].cpu().tolist()\n",
    "    solution = tokenizer.decode(full_sequence, skip_special_tokens=True)\n",
    "    return solution, attentions_per_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64874833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`GPT2SdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "example_problem = test_data.iloc[0][\"problem\"]\n",
    "solution, attentions = generate_with_attentions(example_problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2be06c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem: 35 * 28 =\n",
      "Generated Solution: 35 * 28 = 940\n"
     ]
    }
   ],
   "source": [
    "print(f\"Problem: {example_problem}\")\n",
    "print(f\"Generated Solution: {solution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93c1fc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "print(attentions[2][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7f39f",
   "metadata": {},
   "source": [
    "Run the model and extract the attention and calculate the reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71f98cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut off the not used solution part\n",
    "def trim_solution(solution):\n",
    "    # Find the index of the split token in the solution\n",
    "    split_token = \" =\"\n",
    "    split_index = solution.find(split_token)\n",
    "    if split_index != -1:\n",
    "        # Trim the solution to only include the part after the problem\n",
    "        trimmed_solution = solution[split_index + len(split_token):].strip()\n",
    "    else:\n",
    "        trimmed_solution = solution.strip()\n",
    "    return trimmed_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfe60efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data for the dataset\n",
    "attention_data = []\n",
    "reward_data = []\n",
    "\n",
    "# generate attention reward pairs\n",
    "for i in range(100): \n",
    "    # get problem, model solution and attention weights\n",
    "    example_problem = test_data.iloc[i][\"problem\"]\n",
    "    solution, attentions = generate_with_attentions(example_problem)\n",
    "    # trim solution for reward calculation\n",
    "    solution = trim_solution(solution)\n",
    "    # get the real solution and calculate the reward\n",
    "    example_solution = test_data.iloc[i][\"solution\"]\n",
    "    diff = torch.tensor([abs(float(solution) - float(example_solution) + 1e-6)])\n",
    "    reward = -torch.log(diff).item()\n",
    "\n",
    "    # collect for dataset\n",
    "    attention_data.append(attentions)\n",
    "    reward_data.append(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6753d91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(attention_data))\n",
    "print(len(reward_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57680866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put data into a dataset for training\n",
    "# TODO for attention extract number of layer and solution lenght\n",
    "# TODO Return size with getitem\n",
    "\n",
    "class AttentionDataset:\n",
    "    def __init__(self, attentions, rewards):\n",
    "        self.attentions = attentions\n",
    "        self.rewards = rewards\n",
    "        self.dataset_size = len(rewards)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.attentions[idx], self.rewards[idx]\n",
    "    \n",
    "def attention_collate_fn(batch):\n",
    "    attentions, rewards = zip(*batch)\n",
    "    return list(attentions), torch.tensor(rewards)\n",
    "    \n",
    "test_attention_dataset = AttentionDataset(attention_data, reward_data)\n",
    "test_attention_loader = DataLoader(test_attention_dataset, batch_size=32, shuffle=False, collate_fn=attention_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5ba6fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n",
      "Rewards: tensor([-3.6889e+00, -2.8332e+00, -2.9957e+00,  1.3816e+01,  1.3816e+01,\n",
      "        -1.3863e+00,  1.3816e+01, -8.6958e+00, -3.2958e+00, -4.4998e+00,\n",
      "        -5.1930e+00, -4.6444e+00, -5.0999e+00, -4.7875e+00, -6.2046e+00,\n",
      "        -6.9315e-01, -6.7662e+00, -6.0403e+00, -1.7918e+00, -4.6052e+00,\n",
      "        -7.2049e+00, -4.3307e+00, -9.5367e-07,  1.3816e+01, -4.4543e+00,\n",
      "        -2.3026e+00, -6.9315e-01,  1.3816e+01, -4.3820e+00, -6.9315e-01,\n",
      "        -4.0943e+00, -3.9703e+00])\n",
      "Attention shape: torch.Size([1, 12, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "for batch in test_attention_loader:\n",
    "    attentions, rewards = batch\n",
    "    # Process the batch as needed\n",
    "    print(f\"Batch size: {len(attentions)}\")\n",
    "    print(f\"Rewards: {rewards}\")\n",
    "    print(f\"Attention shape: {attentions[0][0][0].shape}\")\n",
    "    break  # Just to check the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c59dfef",
   "metadata": {},
   "source": [
    "Saving the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8560c31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset size: 100\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# TODO: optinally change to json format to make data human readable\n",
    "# Save the dataset to a file\n",
    "with open(\"attention_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump((attention_data, reward_data), f)\n",
    "# Load the dataset from a file\n",
    "with open(\"attention_dataset.pkl\", \"rb\") as f:\n",
    "    loaded_attention_data, loaded_reward_data = pickle.load(f)\n",
    "print(f\"Loaded dataset size: {len(loaded_reward_data)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adverserialAlignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
