{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c8b68c",
   "metadata": {},
   "source": [
    "Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "454f1ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.utils import from_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0552afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for printing and plotting graphs\n",
    "def print_graph_details(G):\n",
    "    print(\"Nodes:\")\n",
    "    for node, data in G.nodes(data=True):\n",
    "        print(f\"{node}: {data}\")\n",
    "    \n",
    "    print(\"\\nEdges:\")\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        print(f\"{u} -> {v}: {data}\")\n",
    "\n",
    "# Function to plot the graph\n",
    "def plot_graph(G):\n",
    "    pos = nx.spring_layout(G)  # Layout for visualization\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    nx.draw(G, pos, with_labels=True, node_size=500, node_color=\"skyblue\", font_size=10, font_weight=\"bold\", edge_color=\"gray\")\n",
    "    plt.title('Attention Graph')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c14a4c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data using pickle\n",
    "with open(\"attention_dataset.pkl\", \"rb\") as f:\n",
    "    \"\"\"\n",
    "    attention_data: dimension (num_samples, iteration, layer, \n",
    "                                1 #batch during generation, \n",
    "                                num_heads, seq_len, seq_len)\n",
    "    reward_data: list of rewards (num_samples)\n",
    "    \"\"\"\n",
    "    attention_data, reward_data = pickle.load(f)\n",
    "\n",
    "\n",
    "# Create the graph structure from the attention weights\n",
    "def attention_to_graph(attention):\n",
    "    # Get the number of nodes\n",
    "    n = attention.shape[-1] # number of tokens\n",
    "\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes from attention\n",
    "    for i in range(n):\n",
    "        # TODO: weight dependend on the number of the tokens\n",
    "        G.add_node(f'token_{i}', weight=attention[i, i])\n",
    "    \n",
    "    # Add edges from attention\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if (j < i): # attention masking\n",
    "                G.add_edge(f'token_{i}', f'token_{j}', weight=attention[i, j])\n",
    "\n",
    "    # TODO: Check for further aggregation for transformer input\n",
    "    return G\n",
    "\n",
    "# AttentionDataset class\n",
    "class AttentionDataset:\n",
    "    def __init__(self, attentions, rewards):\n",
    "        self.attentions = attentions\n",
    "        self.rewards = rewards\n",
    "        self.dataset_size = len(rewards)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.attentions[idx], self.rewards[idx]\n",
    "    \n",
    "def attention_collate_fn(batch):\n",
    "    # get data from file and convert to format for PyTorch Geometric\n",
    "    attentions, rewards = zip(*batch)\n",
    "\n",
    "    # rewards\n",
    "    rewards = torch.tensor(rewards)  # Convert rewards to tensor\n",
    "\n",
    "    # attention\n",
    "    # data: (num_samples, iteration, layer, 1, num_heads, seq_len, seq_len), rewards: (num_samples)\n",
    "    attention_data_batch = []\n",
    "    for attention_iteration in attentions:\n",
    "        attention_data_step = []\n",
    "        for attention_layer in attention_iteration:\n",
    "            # attention_layer: (layer, 1, num_heads, seq_len, seq_len)\n",
    "            attention_data_layer = []\n",
    "            for attention_batch in attention_layer: \n",
    "                for attention_head in attention_batch: # TODO: Change data structure to remove the singleton batch dimension\n",
    "                    attention_data_head = []\n",
    "                    for attention in attention_head:  \n",
    "                        # attention_head: (num_heads, seq_len, seq_len)\n",
    "                        G = attention_to_graph(attention)\n",
    "                        pyg_graph = from_networkx(G, group_node_attrs=['weight'], group_edge_attrs=['weight'])\n",
    "                        attention_data_head.append(pyg_graph) # Put G for NetworkX graph\n",
    "                attention_data_layer.append(Batch.from_data_list(attention_data_head))\n",
    "            attention_data_step.append(attention_data_layer)\n",
    "        attention_data_batch.append(attention_data_step)\n",
    "\n",
    "    return attention_data_batch, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "171a8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_dataset = AttentionDataset(attention_data, reward_data)\n",
    "attention_loader = DataLoader(attention_dataset, batch_size=32, shuffle=False, collate_fn=attention_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ac3307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch attentions: 32\n",
      "Batch attentions: 3\n",
      "Batch attentions: 12\n",
      "Batch attentions: 12\n",
      "Batch attentions: 3\n"
     ]
    }
   ],
   "source": [
    "# Check Data dimensions\n",
    "for batch in attention_loader:\n",
    "    attentions, rewards = batch\n",
    "    print(\"Batch attentions:\", len(attentions))  # Check the number of attention graphs in the batch\n",
    "    print(\"Batch attentions:\", len(attentions[0]))\n",
    "    print(\"Batch attentions:\", len(attentions[0][0]))\n",
    "    print(\"Batch attentions:\", len(attentions[0][0][0]))\n",
    "    print(\"Batch attentions:\", len(attentions[0][0][0][0]))\n",
    "    break  # Just to test the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ef3ea4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 6], x=[4, 1], edge_attr=[6, 1])\n",
      "Node features (x): tensor([[1.0000],\n",
      "        [0.2059],\n",
      "        [0.2359],\n",
      "        [0.0536]])\n",
      "Edge index: tensor([[1, 2, 2, 3, 3, 3],\n",
      "        [0, 0, 1, 0, 1, 2]])\n",
      "Edge attributes: tensor([[0.7941],\n",
      "        [0.5216],\n",
      "        [0.2425],\n",
      "        [0.4180],\n",
      "        [0.1817],\n",
      "        [0.3467]])\n",
      "Label (y): None\n",
      "Number of nodes: 4\n",
      "Number of edges: 6\n",
      "x shape: torch.Size([4, 1])\n",
      "edge_index shape: torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "# Test pytorch geometric graph\n",
    "for batch in attention_loader:\n",
    "    attentions, rewards = batch\n",
    "    pyg_graph = attentions[0][0][0][2]  # Get the first graph\n",
    "    print(pyg_graph)\n",
    "    print(\"Node features (x):\", pyg_graph.x)\n",
    "    print(\"Edge index:\", pyg_graph.edge_index)\n",
    "    print(\"Edge attributes:\", pyg_graph.edge_attr)\n",
    "    print(\"Label (y):\", getattr(pyg_graph, 'y', None))\n",
    "    print(\"Number of nodes:\", pyg_graph.num_nodes)\n",
    "    print(\"Number of edges:\", pyg_graph.num_edges)\n",
    "\n",
    "    if pyg_graph.x is not None:\n",
    "        print(\"x shape:\", pyg_graph.x.shape)\n",
    "    print(\"edge_index shape:\", pyg_graph.edge_index.shape)\n",
    "\n",
    "    break  # Just to test the first graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9c6f79",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5d490f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the GNN model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch_geometric.utils as pyg_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9b563a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the GNN model for aggregating attention graphs\n",
    "\n",
    "It extracts information through message passing and aggregation\n",
    "which is passed to the rest of the model.\n",
    "This is mostly used to extract information and reduce to linear dimensionality\n",
    "\"\"\"\n",
    "class AggregationNetwork(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, embedding_dim=2, dropout=0.2, adj_dropout=0.2): # TODO: hyperparameter tuning for dropout\n",
    "        super(AggregationNetwork, self).__init__()\n",
    "        self.conv1 = GCNConv(1, hidden_dim) # each node has a single feature (weight)\n",
    "        self.conv2 = GCNConv(hidden_dim, embedding_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.adj_dropout = adj_dropout\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr  \n",
    "        edge_index, edge_mask = pyg_utils.dropout_edge(edge_index, p=self.adj_dropout, training=self.training)\n",
    "\n",
    "        if edge_weight is not None:\n",
    "            edge_weight = edge_weight[edge_mask]\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ae4837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the Compression Network to compress data from the AggregationNetwork\n",
    "\n",
    "Compresses the embedding from the AggregationNetworks (Heads, Layers)\n",
    "into a smaller representation. This is then passed to the adversarial transformer model.\n",
    "\"\"\"\n",
    "class CompressionNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, compressed_dim, dropout=0.1):\n",
    "        super(CompressionNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, compressed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d68c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Define the AggregationEncoderTransformer used to link the different layers\n",
    "\n",
    "Encoder only transformer that processes the aggregated embeddings, to link them togheter\n",
    "and further compress a signle network state.\n",
    "\"\"\"\n",
    "class AggregationEncoderTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super(AggregationEncoderTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)  # Output layer\n",
    "        \n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, 100, hidden_dim))  # Positional encoding\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + self.pos_encoder[:, :x.size(1), :]  # Add positional encoding\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6805a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Define the AttentionToRewardEncoder used to predict rewards\n",
    "\n",
    "This encoder processes the attention features and predicts a reward based on them.\n",
    "Used to predict the reward over multiple iterations of the primary Network.\n",
    "\"\"\"\n",
    "class AttentionToRewardEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, num_head=8, num_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super(AttentionToRewardEncoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)  # Project attention features\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_head , \n",
    "                                       dim_feedforward=dim_feedforward, \n",
    "                                       dropout=dropout, batch_first=True),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)  # Predict reward\n",
    "\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, 100, hidden_dim))  # Positional encoding\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  \n",
    "        x = self.dropout(x)\n",
    "        x = x + self.pos_encoder[:, :x.size(1), :]  # Add positional info\n",
    "        x = self.transformer_encoder(x)  # No causal mask needed\n",
    "        x = x.mean(dim=1)  # Pool over token representations (global understanding)\n",
    "        x = self.fc_out(x)  # Predict token logits  \n",
    "        \n",
    "        return x  # Output shape: (reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f292ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullAdversarialAlignmentModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_iterations,\n",
    "        num_layers,\n",
    "        num_heads,\n",
    "\n",
    "        gnn_hidden_dim,\n",
    "        gnn_embedding_dim,\n",
    "\n",
    "        compression_hidden_dim,\n",
    "        compression_dim,\n",
    "\n",
    "        agg_hidden_dim,\n",
    "        agg_heads,\n",
    "        agg_layers,\n",
    "        \n",
    "        reward_hidden_dim,\n",
    "        reward_heads,\n",
    "        reward_layers,\n",
    "        reward_ff_dim,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super(FullAdversarialAlignmentModel, self).__init__()\n",
    "\n",
    "        # Shared GNN across all heads, layers, iterations\n",
    "        self.gnn = AggregationNetwork(\n",
    "            hidden_dim=gnn_hidden_dim,\n",
    "            embedding_dim=gnn_embedding_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Separate compression for each layer (compress across heads)\n",
    "        self.compressors = nn.ModuleList([\n",
    "            CompressionNetwork(\n",
    "                input_dim=gnn_embedding_dim * num_heads,\n",
    "                hidden_dim=compression_hidden_dim,\n",
    "                compressed_dim=compression_dim,\n",
    "                dropout=dropout\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Shared AggregationEncoder for linking compressed layers inside each iteration\n",
    "        self.aggregation_encoder = AggregationEncoderTransformer(\n",
    "            input_dim=compression_dim * num_layers,  # Concatenate all compressed layers\n",
    "            output_dim=agg_hidden_dim,  # Output dimension for each iteration summary\n",
    "            hidden_dim=agg_hidden_dim,\n",
    "            num_heads=agg_heads,\n",
    "            num_layers=agg_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Final reward predictor processing the sequence of iteration summaries\n",
    "        self.reward_predictor = AttentionToRewardEncoder(\n",
    "            input_dim=agg_hidden_dim * num_iterations,  # Concatenate all iteration summaries\n",
    "            hidden_dim=reward_hidden_dim,\n",
    "            num_head=reward_heads,\n",
    "            num_layers=reward_layers,\n",
    "            dim_feedforward=reward_ff_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, attention_graph_batches):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        attention_graph_batches : list of lists of lists\n",
    "            [iteration][layer][head] -> each element is a torch_geometric Batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            [batch_size, 1] reward predictions\n",
    "        \"\"\"\n",
    "\n",
    "        iteration_embeddings = []\n",
    "        for iteration_layers in attention_graph_batches:\n",
    "            \n",
    "            layer_embeddings = []\n",
    "            for layer_idx, layer_heads in enumerate(iteration_layers):\n",
    "                \n",
    "                head_embeddings = []\n",
    "                for head_graph in layer_heads:\n",
    "                    # Shared GNN across all\n",
    "                    gnn_out = self.gnn(head_graph)  # [batch_size, gnn_embedding_dim]\n",
    "                    head_embeddings.append(gnn_out)\n",
    "\n",
    "                # Concat all heads for this layer\n",
    "                head_concat = torch.cat(head_embeddings, dim=-1)  # [batch_size, gnn_embedding_dim * num_heads]\n",
    "\n",
    "                # Compress layer representation with layer-specific compressor\n",
    "                compressed = self.compressors[layer_idx](head_concat)  # [batch_size, compression_dim]\n",
    "                layer_embeddings.append(compressed)  # keep sequence dimension\n",
    "\n",
    "            # Sequence of compressed layers for this iteration\n",
    "            layer_seq = torch.cat(layer_embeddings, dim=1)  # [batch_size, num_layers, compression_dim]\n",
    "            layer_seq_flat = layer_seq.view(layer_seq.size(0), -1)  # [batch_size, num_layers * compression_dim]\n",
    "            iter_encoded = self.aggregation_encoder(layer_seq_flat.unsqueeze(1))  # [batch_size, 1, agg_hidden_dim]\n",
    "\n",
    "            # Pool over layers (mean) to get single iteration summary\n",
    "            iter_summary = iter_encoded.mean(dim=1).unsqueeze(1)  # [batch_size, 1, agg_hidden_dim]\n",
    "            iteration_embeddings.append(iter_summary)\n",
    "\n",
    "        \n",
    "        # Stack all iteration summaries into a sequence\n",
    "        iteration_seq = torch.cat(iteration_embeddings, dim=1)  # [batch_size, num_iterations, agg_hidden_dim]\n",
    "        iteration_seq_flat = iteration_seq.view(iteration_seq.size(0), -1)  # [batch_size, num_iterations * agg_hidden_dim]\n",
    "        # Final reward prediction\n",
    "        reward = self.reward_predictor(iteration_seq_flat.unsqueeze(1))  # [batch_size, 1, input_dim]\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66b75bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullAdversarialAlignmentModel_Padding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_iterations,\n",
    "        num_layers,\n",
    "        num_heads,\n",
    "\n",
    "        gnn_hidden_dim,\n",
    "        gnn_embedding_dim,\n",
    "\n",
    "        compression_hidden_dim,\n",
    "        compression_dim,\n",
    "\n",
    "        agg_hidden_dim,\n",
    "        agg_heads,\n",
    "        agg_layers,\n",
    "        \n",
    "        reward_hidden_dim,\n",
    "        reward_heads,\n",
    "        reward_layers,\n",
    "        reward_ff_dim,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super(FullAdversarialAlignmentModel_Padding, self).__init__()\n",
    "\n",
    "        # needed variables\n",
    "        self.num_iterations = num_iterations # used for padding\n",
    "\n",
    "        # Shared GNN across all heads, layers, iterations\n",
    "        self.gnn = AggregationNetwork(\n",
    "            hidden_dim=gnn_hidden_dim,\n",
    "            embedding_dim=gnn_embedding_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Separate compression for each layer (compress across heads)\n",
    "        self.compressors = nn.ModuleList([\n",
    "            CompressionNetwork(\n",
    "                input_dim=gnn_embedding_dim * num_heads,\n",
    "                hidden_dim=compression_hidden_dim,\n",
    "                compressed_dim=compression_dim,\n",
    "                dropout=dropout\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Shared AggregationEncoder for linking compressed layers inside each iteration\n",
    "        self.aggregation_encoder = AggregationEncoderTransformer(\n",
    "            input_dim=compression_dim * num_layers,  # Concatenate all compressed layers\n",
    "            output_dim=agg_hidden_dim,  # Output dimension for each iteration summary\n",
    "            hidden_dim=agg_hidden_dim,\n",
    "            num_heads=agg_heads,\n",
    "            num_layers=agg_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Final reward predictor processing the sequence of iteration summaries\n",
    "        self.reward_predictor = AttentionToRewardEncoder(\n",
    "            input_dim=agg_hidden_dim * num_iterations,  # Concatenate all iteration summaries\n",
    "            hidden_dim=reward_hidden_dim,\n",
    "            num_head=reward_heads,\n",
    "            num_layers=reward_layers,\n",
    "            dim_feedforward=reward_ff_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, attention_graph_batches):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        attention_graph_batches : list of lists of lists\n",
    "            [iteration][layer][head] -> each element is a torch_geometric Batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            [batch_size, 1] reward predictions\n",
    "        \"\"\"\n",
    "\n",
    "        iteration_embeddings = []\n",
    "        for iteration_layers in attention_graph_batches:\n",
    "            layer_embeddings = []\n",
    "            for layer_idx, layer_heads in enumerate(iteration_layers):\n",
    "                \n",
    "                head_embeddings = []\n",
    "                for head_graph in layer_heads:\n",
    "                    # Shared GNN across all\n",
    "                    gnn_out = self.gnn(head_graph)  # [batch_size, gnn_embedding_dim]\n",
    "                    head_embeddings.append(gnn_out)\n",
    "\n",
    "                # Concat all heads for this layer\n",
    "                head_concat = torch.cat(head_embeddings, dim=-1)  # [batch_size, gnn_embedding_dim * num_heads]\n",
    "                #print(\"head_concat shape:\", head_concat.shape)\n",
    "                # Compress layer representation with layer-specific compressor\n",
    "                compressed = self.compressors[layer_idx](head_concat)  # [batch_size, compression_dim]\n",
    "                #print(\"compressed shape:\", compressed.shape)\n",
    "                layer_embeddings.append(compressed)  # keep sequence dimension\n",
    "\n",
    "            # Sequence of compressed layers for this iteration\n",
    "            #print(\"layer_embeddings[0] shape:\", layer_embeddings[0].shape)\n",
    "            #print(\"layer_embeddings length:\", len(layer_embeddings))\n",
    "            layer_seq = torch.stack(layer_embeddings, dim=0)  # [batch_size, num_layers, compression_dim]\n",
    "            #print(\"layer_seq shape:\", layer_seq.shape)\n",
    "            layer_seq_flat = layer_seq.view(layer_seq.size(0), -1)  # [batch_size, num_layers * compression_dim]\n",
    "            #print(\"layer_seq_flat shape:\", layer_seq_flat.shape)\n",
    "            iter_encoded = self.aggregation_encoder(layer_seq_flat.unsqueeze(1))  # [batch_size, 1, agg_hidden_dim]\n",
    "\n",
    "            # Pool over layers (mean) to get single iteration summary\n",
    "            iter_summary = iter_encoded.mean(dim=1).unsqueeze(1)  # [batch_size, 1, agg_hidden_dim]\n",
    "            iteration_embeddings.append(iter_summary)\n",
    "\n",
    "        # Padding\n",
    "        dummy = torch.zeros((1, 1, iteration_embeddings[0].size(2)), device=iteration_embeddings[0].device)\n",
    "        for i in range(len(iteration_embeddings)):\n",
    "            num_missing = self.num_iterations - iteration_embeddings[i].size(0)\n",
    "            #print(\"num_missing:\", num_missing)\n",
    "            if num_missing > 0:\n",
    "                # Append num_missing copies to the tensor\n",
    "                pad = dummy.repeat(num_missing, 1, 1)\n",
    "                #print(\"pad shape:\", pad.shape)\n",
    "                iteration_embeddings[i] = torch.cat([pad, iteration_embeddings[i]], dim=0)\n",
    "            \n",
    "        # for i, iteration_embedding in enumerate(iteration_embeddings):\n",
    "        #     print(\"Iteration_embeddings shape:\", i, iteration_embedding.shape)\n",
    "        \n",
    "        # Stack all iteration summaries into a sequence\n",
    "        #print(\"iteration_embeddings length:\", len(iteration_embeddings))\n",
    "        #print(\"iteration_embeddings[0] shape:\", iteration_embeddings[0].shape)\n",
    "        iteration_seq = torch.cat(iteration_embeddings, dim=1)  # [batch_size, num_iterations, agg_hidden_dim]\n",
    "        #print(\"iteration_seq shape:\", iteration_seq.shape)\n",
    "        iteration_seq_flat = iteration_seq.view(iteration_seq.size(1), -1)  # [batch_size, num_iterations * agg_hidden_dim]\n",
    "        #print(\"iteration_seq_flat shape:\", iteration_seq_flat.shape)\n",
    "        # Final reward prediction\n",
    "        reward = self.reward_predictor(iteration_seq_flat.unsqueeze(1))  # [batch_size, 1, input_dim]\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41c43019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x40 and 8x4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 52\u001b[0m\n\u001b[0;32m     33\u001b[0m model \u001b[38;5;241m=\u001b[39m FullAdversarialAlignmentModel_Padding(\n\u001b[0;32m     34\u001b[0m     num_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     35\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39mnum_layers,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# === Forward pass ===\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_graph_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal reward prediction shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Expect [batch_size, 1]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[13], line 105\u001b[0m, in \u001b[0;36mFullAdversarialAlignmentModel_Padding.forward\u001b[1;34m(self, attention_graph_batches)\u001b[0m\n\u001b[0;32m    103\u001b[0m layer_seq_flat \u001b[38;5;241m=\u001b[39m layer_seq\u001b[38;5;241m.\u001b[39mview(layer_seq\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size, num_layers * compression_dim]\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m#print(\"layer_seq_flat shape:\", layer_seq_flat.shape)\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m iter_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregation_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_seq_flat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, 1, agg_hidden_dim]\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Pool over layers (mean) to get single iteration summary\u001b[39;00m\n\u001b[0;32m    108\u001b[0m iter_summary \u001b[38;5;241m=\u001b[39m iter_encoded\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size, 1, agg_hidden_dim]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[10], line 21\u001b[0m, in \u001b[0;36mAggregationEncoderTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder[:, :x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), :]  \u001b[38;5;66;03m# Add positional encoding\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x40 and 8x4)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "# === Dummy data generator ===\n",
    "def generate_dummy_graph_batch(batch_size, num_nodes, feature_dim=1):\n",
    "    # Create multiple small graphs in a single batch\n",
    "    data_list = []\n",
    "    for _ in range(batch_size):\n",
    "        x = torch.rand((num_nodes, feature_dim))  # Node features\n",
    "        edge_index = torch.randint(0, num_nodes, (2, num_nodes * 2))  # Random edges\n",
    "        data_list.append(Data(x=x, edge_index=edge_index))\n",
    "    return Batch.from_data_list(data_list)\n",
    "\n",
    "# === Build dummy inputs matching your architecture ===\n",
    "batch_size = 10\n",
    "num_iterations = 2\n",
    "num_layers = 2\n",
    "num_heads = 2\n",
    "\n",
    "# Build attention_graph_batches[iteration][layer][head]\n",
    "attention_graph_batches = []\n",
    "for _ in range(num_iterations):\n",
    "    iteration_layers = []\n",
    "    for _ in range(num_layers):\n",
    "        layer_heads = []\n",
    "        for _ in range(num_heads):\n",
    "            head_batch = generate_dummy_graph_batch(batch_size, num_nodes=5)\n",
    "            layer_heads.append(head_batch)\n",
    "        iteration_layers.append(layer_heads)\n",
    "    attention_graph_batches.append(iteration_layers)\n",
    "\n",
    "# === Build the model ===\n",
    "model = FullAdversarialAlignmentModel_Padding(\n",
    "    num_iterations=5,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    gnn_hidden_dim=16,\n",
    "    gnn_embedding_dim=8,\n",
    "    compression_hidden_dim=32,\n",
    "    compression_dim=4,\n",
    "    agg_hidden_dim=4,\n",
    "    agg_heads=2,\n",
    "    agg_layers=2,\n",
    "    reward_hidden_dim=24,\n",
    "    reward_heads=2,\n",
    "    reward_layers=2,\n",
    "    reward_ff_dim=64,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# === Forward pass ===\n",
    "reward = model(attention_graph_batches)\n",
    "print(f\"\\nFinal reward prediction shape: {reward.shape}\")  # Expect [batch_size, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572562a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d8645ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 6], x=[4, 1], edge_attr=[6, 1])\n",
      "Node features (x): tensor([[1.0000],\n",
      "        [0.0703],\n",
      "        [0.0172],\n",
      "        [0.0466]])\n",
      "Edge index: tensor([[1, 2, 2, 3, 3, 3],\n",
      "        [0, 0, 1, 0, 1, 2]])\n",
      "Edge attributes: tensor([[0.9297],\n",
      "        [0.7762],\n",
      "        [0.2066],\n",
      "        [0.4831],\n",
      "        [0.3801],\n",
      "        [0.0902]])\n",
      "Label (y): None\n",
      "Number of nodes: 4\n",
      "Number of edges: 6\n",
      "x shape: torch.Size([4, 1])\n",
      "edge_index shape: torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "for batch in attention_loader:\n",
    "    attentions, rewards = batch\n",
    "    pyg_graph = attentions[0][0][0][0]  # Get the first graph\n",
    "    print(pyg_graph)\n",
    "    print(\"Node features (x):\", pyg_graph.x)\n",
    "    print(\"Edge index:\", pyg_graph.edge_index)\n",
    "    print(\"Edge attributes:\", pyg_graph.edge_attr)\n",
    "    print(\"Label (y):\", getattr(pyg_graph, 'y', None))\n",
    "    print(\"Number of nodes:\", pyg_graph.num_nodes)\n",
    "    print(\"Number of edges:\", pyg_graph.num_edges)\n",
    "\n",
    "    if pyg_graph.x is not None:\n",
    "        print(\"x shape:\", pyg_graph.x.shape)\n",
    "    print(\"edge_index shape:\", pyg_graph.edge_index.shape)\n",
    "\n",
    "    break  # Just to test the first graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540826c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_embeddings length: 32\n",
      "iteration_embeddings[0] shape: torch.Size([5, 1, 4])\n",
      "iteration_seq shape: torch.Size([5, 32, 4])\n",
      "iteration_seq_flat shape: torch.Size([32, 20])\n",
      "Predicted reward shape: torch.Size([32, 1])\n",
      "Predicted reward: [-0.29744077 -0.34963167 -0.52768946 -0.43813324 -0.33486015 -0.6731272\n",
      " -0.73075515 -0.7117985  -0.7345962  -0.60961884 -0.5868688  -0.28745365\n",
      " -0.49931788 -0.6043281  -0.6093149  -0.63385195 -0.48177934 -0.47170025\n",
      " -0.6744481  -0.44590962 -0.8785878  -0.6509539  -0.76292455 -0.603332\n",
      " -0.61975515 -0.75231326 -0.87337506 -0.6542589  -0.49191695 -0.84706193\n",
      " -0.72784865 -0.9169785 ]\n",
      "True reward: [-3.7376697e+00 -6.8341088e+00  1.3815511e+01 -6.9314671e-01\n",
      " -8.5149908e+00 -6.1158919e+00  1.3815511e+01 -4.1588831e+00\n",
      " -3.8712010e+00  1.3815511e+01 -4.4426513e+00 -6.1136823e+00\n",
      "  1.3815511e+01 -1.7917596e+00 -5.8051348e+00 -4.7874918e+00\n",
      " -2.9957323e+00 -5.7037826e+00 -4.3820267e+00 -6.8330317e+00\n",
      " -3.5263605e+00 -2.3025851e+00 -3.4011974e+00  1.3815511e+01\n",
      " -6.9314766e-01 -7.8766384e+00  1.3815511e+01 -5.5834961e+00\n",
      " -3.8286414e+00 -4.3694477e+00 -1.6094381e+00  1.0132795e-06]\n",
      "iteration_embeddings length: 32\n",
      "iteration_embeddings[0] shape: torch.Size([5, 1, 4])\n",
      "iteration_seq shape: torch.Size([5, 32, 4])\n",
      "iteration_seq_flat shape: torch.Size([32, 20])\n",
      "Predicted reward shape: torch.Size([32, 1])\n",
      "Predicted reward: [-0.45720077 -0.5236099  -0.5693046  -0.6560548  -0.44913512 -0.11905655\n",
      " -0.5159016  -0.5717149  -0.6436772  -0.3504206  -0.46158308 -0.3844424\n",
      " -0.35688084 -0.40514517 -0.7541017  -0.58338356 -0.50140774 -0.5697205\n",
      " -0.5766279  -0.38500053 -0.6952224  -1.005807   -0.85157377 -0.6671595\n",
      " -0.74774325 -0.6630018  -0.46598697 -0.48295146 -0.3489011  -0.6202728\n",
      " -0.4351707  -0.7264513 ]\n",
      "True reward: [13.815511   -0.6931467  -1.7917596  -1.7917593  -4.644391   -4.9698133\n",
      " -3.637586   -0.69314766 -4.406719   -5.192957   -4.7004805  -6.6333184\n",
      " -0.69314766 13.815511   -3.7841897  -3.8286414  -3.871201   -3.3322046\n",
      " 13.815511   13.815511   -1.6094381  13.815511   -3.3322043  13.815511\n",
      " -1.7917593  -8.478661   -3.6109178  -3.0910423  -4.5325994  -3.9318256\n",
      " -7.650645   -2.8903718 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 23\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m FullAdversarialAlignmentModel_Padding(\n\u001b[0;32m      5\u001b[0m     num_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      6\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Get one batch from the attention_loader\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattention_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# attentions: [batch, iteration, layer, head, graph]\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# print(\"Batch_size\", len(attentions))  # Check the number of attention graphs in the batch\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# print(\"num_iteration:\", len(attentions[0]))\u001b[39;49;00m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# print(\"num_layers:\", len(attentions[0][0]))\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# print(\"num_heads:\", len(attentions[0][0][0]))\u001b[39;49;00m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Forward pass through the model \u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 68\u001b[0m, in \u001b[0;36mattention_collate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attention \u001b[38;5;129;01min\u001b[39;00m attention_head:  \n\u001b[0;32m     66\u001b[0m         \u001b[38;5;66;03m# attention_head: (num_heads, seq_len, seq_len)\u001b[39;00m\n\u001b[0;32m     67\u001b[0m         G \u001b[38;5;241m=\u001b[39m attention_to_graph(attention)\n\u001b[1;32m---> 68\u001b[0m         pyg_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_networkx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_node_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_edge_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m         attention_data_head\u001b[38;5;241m.\u001b[39mappend(pyg_graph) \u001b[38;5;66;03m# Put G for NetworkX graph\u001b[39;00m\n\u001b[0;32m     70\u001b[0m attention_data_layer\u001b[38;5;241m.\u001b[39mappend(Batch\u001b[38;5;241m.\u001b[39mfrom_data_list(attention_data_head))\n",
      "File \u001b[1;32mc:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\torch_geometric\\utils\\convert.py:291\u001b[0m, in \u001b[0;36mfrom_networkx\u001b[1;34m(G, group_node_attrs, group_edge_attrs)\u001b[0m\n\u001b[0;32m    289\u001b[0m         xs\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m data[key]\n\u001b[1;32m--> 291\u001b[0m     data\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group_edge_attrs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    294\u001b[0m     xs \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Batch\n",
    "\n",
    "# === Build the model ===\n",
    "model = FullAdversarialAlignmentModel_Padding(\n",
    "    num_iterations=5,\n",
    "    num_layers=12,\n",
    "    num_heads=12,\n",
    "    gnn_hidden_dim=16,\n",
    "    gnn_embedding_dim=8,\n",
    "    compression_hidden_dim=32,\n",
    "    compression_dim=4,\n",
    "    agg_hidden_dim=4,\n",
    "    agg_heads=2,\n",
    "    agg_layers=2,\n",
    "    reward_hidden_dim=24,\n",
    "    reward_heads=2,\n",
    "    reward_layers=2,\n",
    "    reward_ff_dim=64,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Get one batch from the attention_loader\n",
    "for batch in attention_loader:\n",
    "    attentions, rewards = batch  # attentions: [batch, iteration, layer, head, graph]\n",
    "    # print(\"Batch_size\", len(attentions))  # Check the number of attention graphs in the batch\n",
    "    # print(\"num_iteration:\", len(attentions[0]))\n",
    "    # print(\"num_layers:\", len(attentions[0][0]))\n",
    "    # print(\"num_heads:\", len(attentions[0][0][0]))\n",
    "    # Forward pass through the model \n",
    "    reward_pred = model(attentions)  # Output: [batch_size, 1]\n",
    "    print(\"Predicted reward shape:\", reward_pred.shape)\n",
    "    print(\"Predicted reward:\", reward_pred.squeeze().detach().cpu().numpy())\n",
    "    print(\"True reward:\", rewards.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4805dfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "wandb: Currently logged in as: leonhard_waibl to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\leonh\\OneDrive\\Dokumente\\a_informatik\\projects\\AdverserialAlignment\\wandb\\run-20250716_095703-ysu1akfg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leonhard_waibl/adversarial-alignment/runs/ysu1akfg' target=\"_blank\">spring-sponge-1</a></strong> to <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment' target=\"_blank\">https://wandb.ai/leonhard_waibl/adversarial-alignment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment/runs/ysu1akfg' target=\"_blank\">https://wandb.ai/leonhard_waibl/adversarial-alignment/runs/ysu1akfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|| 4/4 [00:42<00:00, 10.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 51.752468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|| 4/4 [00:42<00:00, 10.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 52.718700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|| 4/4 [00:41<00:00, 10.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 52.193806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|| 4/4 [00:39<00:00,  9.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Loss: 51.140471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|| 4/4 [00:39<00:00,  9.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Loss: 51.110319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|| 4/4 [00:38<00:00,  9.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Avg Loss: 51.706482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|| 4/4 [00:38<00:00,  9.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Loss: 51.479305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|| 4/4 [00:38<00:00,  9.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Avg Loss: 51.272855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|| 4/4 [00:38<00:00,  9.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Avg Loss: 51.916001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|| 4/4 [00:35<00:00,  8.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Avg Loss: 52.546176\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# == Assume you already have these prepared ==\n",
    "# dataset = AttentionDataset(...)\n",
    "# dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=your_collate_fn)\n",
    "\n",
    "# Your model\n",
    "model = FullAdversarialAlignmentModel_Padding(\n",
    "    num_iterations=5,\n",
    "    num_layers=12,\n",
    "    num_heads=12,\n",
    "    gnn_hidden_dim=16,\n",
    "    gnn_embedding_dim=8,\n",
    "    compression_hidden_dim=32,\n",
    "    compression_dim=4,\n",
    "    agg_hidden_dim=4,\n",
    "    agg_heads=2,\n",
    "    agg_layers=2,\n",
    "    reward_hidden_dim=24,\n",
    "    reward_heads=2,\n",
    "    reward_layers=2,\n",
    "    reward_ff_dim=64,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# === Optionally start wandb ===\n",
    "import wandb\n",
    "wandb.init(project=\"adversarial-alignment\")\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(attention_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        attention_batches, rewards = batch\n",
    "\n",
    "        # Move rewards to device\n",
    "        rewards = rewards.to(device).float()\n",
    "\n",
    "        # Move graphs to device\n",
    "        for iteration_layers in attention_batches:\n",
    "            for layer_heads in iteration_layers:\n",
    "                for i, head_graph in enumerate(layer_heads):\n",
    "                    layer_heads[i] = head_graph.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(attention_batches).squeeze()\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(predictions, rewards)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # === Optionally log to wandb ===\n",
    "        \n",
    "        wandb.log({\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"predictions_mean\": predictions.mean().item(),\n",
    "                    \"predictions_std\": predictions.std().item(),\n",
    "                    \"targets_mean\": rewards.mean().item(),\n",
    "                    \"targets_std\": rewards.std().item()\n",
    "                })\n",
    "        \n",
    "        total_norm = 0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "        wandb.log({\"grad_norm\": total_norm})\n",
    "\n",
    "    avg_loss = epoch_loss / len(attention_loader)\n",
    "    wandb.log({\"epoch_avg_loss\": avg_loss})\n",
    "    print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# === Save final model ===\n",
    "torch.save(model.state_dict(), \"alignment_model_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0290b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: mfr95nkg\n",
      "Sweep URL: https://wandb.ai/leonhard_waibl/adversarial-alignment/sweeps/mfr95nkg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Calling wandb.login() after wandb.init() has no effect.\n",
      "wandb: Agent Starting Run: w2yku2ru with config:\n",
      "wandb: \tagg_hidden_dim: 8\n",
      "wandb: \tcompression_dim: 4\n",
      "wandb: \tdropout: 0.3\n",
      "wandb: \tlr: 0.0001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\leonh\\OneDrive\\Dokumente\\a_informatik\\projects\\AdverserialAlignment\\wandb\\run-20250716_101709-w2yku2ru</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leonhard_waibl/adversarial-alignment/runs/w2yku2ru' target=\"_blank\">dauntless-sweep-1</a></strong> to <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment/sweeps/mfr95nkg' target=\"_blank\">https://wandb.ai/leonhard_waibl/adversarial-alignment/sweeps/mfr95nkg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment' target=\"_blank\">https://wandb.ai/leonhard_waibl/adversarial-alignment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment/sweeps/mfr95nkg' target=\"_blank\">https://wandb.ai/leonhard_waibl/adversarial-alignment/sweeps/mfr95nkg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment/runs/w2yku2ru' target=\"_blank\">https://wandb.ai/leonhard_waibl/adversarial-alignment/runs/w2yku2ru</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run with config: {'agg_hidden_dim': 8, 'compression_dim': 4, 'dropout': 0.3, 'lr': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|| 4/4 [00:36<00:00,  9.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 53.772911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|| 4/4 [00:38<00:00,  9.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 52.901051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|| 4/4 [00:38<00:00,  9.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 54.353225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|| 4/4 [00:37<00:00,  9.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Loss: 53.483410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|| 4/4 [00:38<00:00,  9.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Loss: 53.484097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|| 4/4 [00:36<00:00,  9.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Avg Loss: 51.977777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|| 4/4 [00:36<00:00,  9.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Loss: 53.398236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|| 4/4 [00:37<00:00,  9.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Avg Loss: 53.467681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|| 4/4 [00:39<00:00,  9.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Avg Loss: 53.453395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|| 4/4 [00:39<00:00,  9.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Avg Loss: 53.933036\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch_avg_loss</td><td></td></tr><tr><td>grad_norm</td><td></td></tr><tr><td>loss</td><td></td></tr><tr><td>predictions_mean</td><td></td></tr><tr><td>predictions_std</td><td></td></tr><tr><td>targets_mean</td><td></td></tr><tr><td>targets_std</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch_avg_loss</td><td>53.93304</td></tr><tr><td>grad_norm</td><td>60.34888</td></tr><tr><td>loss</td><td>72.99413</td></tr><tr><td>predictions_mean</td><td>0.18906</td></tr><tr><td>predictions_std</td><td>0.46809</td></tr><tr><td>targets_mean</td><td>-0.26792</td></tr><tr><td>targets_std</td><td>9.42233</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dauntless-sweep-1</strong> at: <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment/runs/w2yku2ru' target=\"_blank\">https://wandb.ai/leonhard_waibl/adversarial-alignment/runs/w2yku2ru</a><br> View project at: <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment' target=\"_blank\">https://wandb.ai/leonhard_waibl/adversarial-alignment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250716_101709-w2yku2ru\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: 89nwh9oj with config:\n",
      "wandb: \tagg_hidden_dim: 4\n",
      "wandb: \tcompression_dim: 16\n",
      "wandb: \tdropout: 0.1\n",
      "wandb: \tlr: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\leonh\\OneDrive\\Dokumente\\a_informatik\\projects\\AdverserialAlignment\\wandb\\run-20250716_102335-89nwh9oj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leonhard_waibl/adversarial-alignment/runs/89nwh9oj' target=\"_blank\">atomic-sweep-2</a></strong> to <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment/sweeps/mfr95nkg' target=\"_blank\">https://wandb.ai/leonhard_waibl/adversarial-alignment/sweeps/mfr95nkg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment' target=\"_blank\">https://wandb.ai/leonhard_waibl/adversarial-alignment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment/sweeps/mfr95nkg' target=\"_blank\">https://wandb.ai/leonhard_waibl/adversarial-alignment/sweeps/mfr95nkg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment/runs/89nwh9oj' target=\"_blank\">https://wandb.ai/leonhard_waibl/adversarial-alignment/runs/89nwh9oj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run with config: {'agg_hidden_dim': 4, 'compression_dim': 16, 'dropout': 0.1, 'lr': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|| 4/4 [00:40<00:00, 10.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 52.401592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|| 4/4 [00:35<00:00,  8.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 51.680532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|| 4/4 [00:39<00:00,  9.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 50.568548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|| 4/4 [00:42<00:00, 10.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Loss: 52.925820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|| 4/4 [00:41<00:00, 10.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Loss: 52.702439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|| 4/4 [00:40<00:00, 10.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Avg Loss: 52.555544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|| 4/4 [00:40<00:00, 10.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Loss: 51.769683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|| 4/4 [00:38<00:00,  9.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Avg Loss: 51.795659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|| 4/4 [00:39<00:00,  9.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Avg Loss: 52.433807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|| 4/4 [00:41<00:00, 10.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Avg Loss: 51.500283\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch_avg_loss</td><td></td></tr><tr><td>grad_norm</td><td></td></tr><tr><td>loss</td><td></td></tr><tr><td>predictions_mean</td><td></td></tr><tr><td>predictions_std</td><td></td></tr><tr><td>targets_mean</td><td></td></tr><tr><td>targets_std</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch_avg_loss</td><td>51.50028</td></tr><tr><td>grad_norm</td><td>21.93045</td></tr><tr><td>loss</td><td>65.01209</td></tr><tr><td>predictions_mean</td><td>-0.73879</td></tr><tr><td>predictions_std</td><td>0.25359</td></tr><tr><td>targets_mean</td><td>-0.26792</td></tr><tr><td>targets_std</td><td>9.42233</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">atomic-sweep-2</strong> at: <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment/runs/89nwh9oj' target=\"_blank\">https://wandb.ai/leonhard_waibl/adversarial-alignment/runs/89nwh9oj</a><br> View project at: <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment' target=\"_blank\">https://wandb.ai/leonhard_waibl/adversarial-alignment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250716_102335-89nwh9oj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: ciwkxekx with config:\n",
      "wandb: \tagg_hidden_dim: 16\n",
      "wandb: \tcompression_dim: 16\n",
      "wandb: \tdropout: 0.3\n",
      "wandb: \tlr: 1e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\leonh\\OneDrive\\Dokumente\\a_informatik\\projects\\AdverserialAlignment\\wandb\\run-20250716_103023-ciwkxekx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leonhard_waibl/adversarial-alignment/runs/ciwkxekx' target=\"_blank\">lilac-sweep-3</a></strong> to <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment/sweeps/mfr95nkg' target=\"_blank\">https://wandb.ai/leonhard_waibl/adversarial-alignment/sweeps/mfr95nkg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment' target=\"_blank\">https://wandb.ai/leonhard_waibl/adversarial-alignment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment/sweeps/mfr95nkg' target=\"_blank\">https://wandb.ai/leonhard_waibl/adversarial-alignment/sweeps/mfr95nkg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/leonhard_waibl/adversarial-alignment/runs/ciwkxekx' target=\"_blank\">https://wandb.ai/leonhard_waibl/adversarial-alignment/runs/ciwkxekx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run with config: {'agg_hidden_dim': 16, 'compression_dim': 16, 'dropout': 0.3, 'lr': 1e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|| 4/4 [00:39<00:00,  9.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 52.564932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|| 4/4 [00:37<00:00,  9.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 52.149525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|| 4/4 [00:34<00:00,  8.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 52.673622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|| 4/4 [00:34<00:00,  8.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Loss: 52.643154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|| 4/4 [00:34<00:00,  8.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Loss: 53.251510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|| 4/4 [00:34<00:00,  8.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Avg Loss: 52.963539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|| 4/4 [00:34<00:00,  8.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Loss: 51.921824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:  25%|       | 1/4 [00:11<00:33, 11.20s/it]"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config\n",
    "\n",
    "        model = FullAdversarialAlignmentModel_Padding(\n",
    "            num_iterations=5,\n",
    "            num_layers=12,\n",
    "            num_heads=12,\n",
    "            gnn_hidden_dim=16,\n",
    "            gnn_embedding_dim=8,\n",
    "            compression_hidden_dim=32,\n",
    "            compression_dim=config.compression_dim,\n",
    "            agg_hidden_dim=config.agg_hidden_dim,\n",
    "            agg_heads=2,\n",
    "            agg_layers=2,\n",
    "            reward_hidden_dim=24,\n",
    "            reward_heads=2,\n",
    "            reward_layers=2,\n",
    "            reward_ff_dim=64,\n",
    "            dropout=config.dropout\n",
    "        ).to(device)\n",
    "\n",
    "        print(\"Starting run with config:\", dict(config))\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            for batch in tqdm(attention_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "                attention_batches, rewards = batch\n",
    "\n",
    "                # Move rewards to device\n",
    "                rewards = rewards.to(device).float()\n",
    "\n",
    "                # Move graphs to device\n",
    "                for iteration_layers in attention_batches:\n",
    "                    for layer_heads in iteration_layers:\n",
    "                        for i, head_graph in enumerate(layer_heads):\n",
    "                            layer_heads[i] = head_graph.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                predictions = model(attention_batches).squeeze()\n",
    "\n",
    "                # Loss\n",
    "                loss = criterion(predictions, rewards)\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                # Backward + optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # === Optionally log to wandb ===\n",
    "                total_norm = 0\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        param_norm = p.grad.data.norm(2)\n",
    "                        total_norm += param_norm.item() ** 2\n",
    "                total_norm = total_norm ** 0.5\n",
    "\n",
    "                wandb.log({\n",
    "                            \"loss\": loss.item(),\n",
    "                            \"predictions_mean\": predictions.mean().detach().item(),\n",
    "                            \"predictions_std\": predictions.std().detach().item(),\n",
    "                            \"targets_mean\": rewards.mean().detach().item(),\n",
    "                            \"targets_std\": rewards.std().detach().item(),\n",
    "                            \"grad_norm\": total_norm,\n",
    "                        })\n",
    "                \n",
    "                \n",
    "            avg_loss = epoch_loss / len(attention_loader)\n",
    "            wandb.log({\"epoch_avg_loss\": avg_loss})\n",
    "            print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# Define sweep\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'name': 'epoch_avg_loss', 'goal': 'minimize'},\n",
    "    'parameters': {\n",
    "        'lr': {'values': [1e-5, 1e-4, 1e-3]},\n",
    "        'dropout': {'values': [0.1, 0.2, 0.3]},\n",
    "        'compression_dim': {'values': [4, 8, 16]},\n",
    "        'agg_hidden_dim': {'values': [4, 8, 16]}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"adversarial-alignment\")\n",
    "wandb.agent(sweep_id, function=train, count=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adverserialAlignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
