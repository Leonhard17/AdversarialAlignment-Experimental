{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c8b68c",
   "metadata": {},
   "source": [
    "Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "454f1ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leonh\\anaconda3\\envs\\adverserialAlignment\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0552afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for printing and plotting graphs\n",
    "def print_graph_details(G):\n",
    "    print(\"Nodes:\")\n",
    "    for node, data in G.nodes(data=True):\n",
    "        print(f\"{node}: {data}\")\n",
    "    \n",
    "    print(\"\\nEdges:\")\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        print(f\"{u} -> {v}: {data}\")\n",
    "\n",
    "# Function to plot the graph\n",
    "def plot_graph(G):\n",
    "    pos = nx.spring_layout(G)  # Layout for visualization\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    nx.draw(G, pos, with_labels=True, node_size=500, node_color=\"skyblue\", font_size=10, font_weight=\"bold\", edge_color=\"gray\")\n",
    "    plt.title('Attention Graph')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c14a4c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data using pickle\n",
    "with open(\"attention_dataset.pkl\", \"rb\") as f:\n",
    "    \"\"\"\n",
    "    attention_data: dimension (num_samples, iteration, layer, \n",
    "                                1 #batch during generation, \n",
    "                                num_heads, seq_len, seq_len)\n",
    "    reward_data: list of rewards (num_samples)\n",
    "    \"\"\"\n",
    "    attention_data, reward_data = pickle.load(f)\n",
    "\n",
    "\n",
    "# Create the graph structure from the attention weights\n",
    "def attention_to_graph(attention):\n",
    "    # Get the number of nodes\n",
    "    n = attention.shape[-1] # number of tokens\n",
    "\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes from attention\n",
    "    for i in range(n):\n",
    "        # TODO: weight dependend on the number of the tokens\n",
    "        G.add_node(f'token_{i}', weight=attention[i, i])\n",
    "    \n",
    "    # Add edges from attention\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if (j < i): # attention masking\n",
    "                G.add_edge(f'token_{i}', f'token_{j}', weight=attention[i, j])\n",
    "\n",
    "    # TODO: Check for further aggregation for transformer input\n",
    "    return G\n",
    "\n",
    "# AttentionDataset class\n",
    "class AttentionDataset:\n",
    "    def __init__(self, attentions, rewards):\n",
    "        self.attentions = attentions\n",
    "        self.rewards = rewards\n",
    "        self.dataset_size = len(rewards)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.attentions[idx], self.rewards[idx]\n",
    "    \n",
    "def attention_collate_fn(batch):\n",
    "    # get data from file and convert to format for PyTorch Geometric\n",
    "    attentions, rewards = zip(*batch)\n",
    "\n",
    "    # rewards\n",
    "    rewards = torch.tensor(rewards)  # Convert rewards to tensor\n",
    "\n",
    "    # attention\n",
    "    # data: (num_samples, iteration, layer, 1, num_heads, seq_len, seq_len), rewards: (num_samples)\n",
    "    attention_data_batch = []\n",
    "    for attention_iteration in attentions:\n",
    "        attention_data_step = []\n",
    "        for attention_layer in attention_iteration:\n",
    "            # attention_layer: (layer, 1, num_heads, seq_len, seq_len)\n",
    "            attention_data_layer = []\n",
    "            for attention_batch in attention_layer: \n",
    "                for attention_head in attention_batch: # TODO: Change data structure to remove the singleton batch dimension\n",
    "                    attention_data_head = []\n",
    "                    for attention in attention_head:  \n",
    "                        # attention_head: (num_heads, seq_len, seq_len)\n",
    "                        G = attention_to_graph(attention)\n",
    "                        pyg_graph = from_networkx(G, group_node_attrs=['weight'], group_edge_attrs=['weight'])\n",
    "                        attention_data_head.append(pyg_graph) # Put G for NetworkX graph\n",
    "                attention_data_layer.append(attention_data_head)\n",
    "            attention_data_step.append(attention_data_layer)\n",
    "        attention_data_batch.append(attention_data_step)\n",
    "\n",
    "    return attention_data_batch, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "171a8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_dataset = AttentionDataset(attention_data, reward_data)\n",
    "attention_loader = DataLoader(attention_dataset, batch_size=32, shuffle=False, collate_fn=attention_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "64ac3307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch attentions: 32\n",
      "Batch attentions: 3\n",
      "Batch attentions: 12\n",
      "Batch attentions: 12\n",
      "Batch attentions: 3\n"
     ]
    }
   ],
   "source": [
    "# Check Data dimensions\n",
    "for batch in attention_loader:\n",
    "    attentions, rewards = batch\n",
    "    print(\"Batch attentions:\", len(attentions))  # Check the number of attention graphs in the batch\n",
    "    print(\"Batch attentions:\", len(attentions[0]))\n",
    "    print(\"Batch attentions:\", len(attentions[0][0]))\n",
    "    print(\"Batch attentions:\", len(attentions[0][0][0]))\n",
    "    print(\"Batch attentions:\", len(attentions[0][0][0][0]))\n",
    "    break  # Just to test the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5ef3ea4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 6], x=[4, 1], edge_attr=[6, 1])\n",
      "Node features (x): tensor([[1.0000],\n",
      "        [0.2059],\n",
      "        [0.2359],\n",
      "        [0.0536]])\n",
      "Edge index: tensor([[1, 2, 2, 3, 3, 3],\n",
      "        [0, 0, 1, 0, 1, 2]])\n",
      "Edge attributes: tensor([[0.7941],\n",
      "        [0.5216],\n",
      "        [0.2425],\n",
      "        [0.4180],\n",
      "        [0.1817],\n",
      "        [0.3467]])\n",
      "Label (y): None\n",
      "Number of nodes: 4\n",
      "Number of edges: 6\n",
      "x shape: torch.Size([4, 1])\n",
      "edge_index shape: torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "# Test pytorch geometric graph\n",
    "for batch in attention_loader:\n",
    "    attentions, rewards = batch\n",
    "    pyg_graph = attentions[0][0][0][2]  # Get the first graph\n",
    "    print(pyg_graph)\n",
    "    print(\"Node features (x):\", pyg_graph.x)\n",
    "    print(\"Edge index:\", pyg_graph.edge_index)\n",
    "    print(\"Edge attributes:\", pyg_graph.edge_attr)\n",
    "    print(\"Label (y):\", getattr(pyg_graph, 'y', None))\n",
    "    print(\"Number of nodes:\", pyg_graph.num_nodes)\n",
    "    print(\"Number of edges:\", pyg_graph.num_edges)\n",
    "\n",
    "    if pyg_graph.x is not None:\n",
    "        print(\"x shape:\", pyg_graph.x.shape)\n",
    "    print(\"edge_index shape:\", pyg_graph.edge_index.shape)\n",
    "\n",
    "    break  # Just to test the first graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9c6f79",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d490f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the GNN model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch_geometric.utils as pyg_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f9b563a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the GNN model for aggregating attention graphs\n",
    "\n",
    "It extracts information through message passing and aggregation\n",
    "which is passed to the rest of the model.\n",
    "This is mostly used to extract information and reduce to linear dimensionality\n",
    "\"\"\"\n",
    "class AggregationNetwork(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, embedding_dim=2, dropout=0.2, adj_dropout=0.2): # TODO: hyperparameter tuning for dropout\n",
    "        super(AggregationNetwork, self).__init__()\n",
    "        self.conv1 = GCNConv(1, hidden_dim) # each node has a single feature (weight)\n",
    "        self.conv2 = GCNConv(hidden_dim, embedding_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.adj_dropout = adj_dropout\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr  \n",
    "        edge_index, edge_mask = pyg_utils.dropout_edge(edge_index, p=self.adj_dropout, training=self.training)\n",
    "\n",
    "        if edge_weight is not None:\n",
    "            edge_weight = edge_weight[edge_mask]\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae4837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the Compression Network to compress data from the AggregationNetwork\n",
    "\n",
    "Compresses the embedding from the AggregationNetworks (Heads, Layers)\n",
    "into a smaller representation. This is then passed to the adversarial transformer model.\n",
    "\"\"\"\n",
    "class CompressionNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, compressed_dim, dropout=0.1):\n",
    "        super(CompressionNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, compressed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d68c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Define the AggregationEncoderTransformer used to link the different layers\n",
    "\n",
    "Encoder only transformer that processes the aggregated embeddings, to link them togheter\n",
    "and further compress a signle network state.\n",
    "\"\"\"\n",
    "class AggregationEncoderTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super(AggregationEncoderTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_dim, input_dim)  # Output layer\n",
    "        \n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, 100, hidden_dim))  # Positional encoding\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + self.pos_encoder[:, :x.size(1), :]  # Add positional encoding\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6805a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Define the AttentionToRewardEncoder used to predict rewards\n",
    "\n",
    "This encoder processes the attention features and predicts a reward based on them.\n",
    "Used to predict the reward over multiple iterations of the primary Network.\n",
    "\"\"\"\n",
    "class AttentionToRewardEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, num_head=8, num_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super(AttentionToRewardEncoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)  # Project attention features\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_head , \n",
    "                                       dim_feedforward=dim_feedforward, \n",
    "                                       dropout=dropout, batch_first=True),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)  # Predict reward\n",
    "\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, 100, hidden_dim))  # Positional encoding\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  \n",
    "        x = self.dropout(x)\n",
    "        x = x + self.pos_encoder[:, :x.size(1), :]  # Add positional info\n",
    "        x = self.transformer_encoder(x)  # No causal mask needed\n",
    "        x = x.mean(dim=1)  # Pool over token representations (global understanding)\n",
    "        x = self.fc_out(x)  # Predict token logits  \n",
    "        \n",
    "        return x  # Output shape: (reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f292ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullAdversarialAlignmentModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_iterations,\n",
    "        num_layers,\n",
    "        num_heads,\n",
    "        gnn_hidden_dim,\n",
    "        gnn_embedding_dim,\n",
    "        compression_hidden_dim,\n",
    "        compression_dim,\n",
    "        agg_hidden_dim,\n",
    "        agg_heads,\n",
    "        agg_layers,\n",
    "        reward_hidden_dim,\n",
    "        reward_heads,\n",
    "        reward_layers,\n",
    "        reward_ff_dim,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super(FullAdversarialAlignmentModel, self).__init__()\n",
    "\n",
    "        # Shared GNN across all heads, layers, iterations\n",
    "        self.gnn = AggregationNetwork(\n",
    "            hidden_dim=gnn_hidden_dim,\n",
    "            embedding_dim=gnn_embedding_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Separate compression for each layer (compress across heads)\n",
    "        self.compressors = nn.ModuleList([\n",
    "            CompressionNetwork(\n",
    "                input_dim=gnn_embedding_dim * num_heads,\n",
    "                hidden_dim=compression_hidden_dim,\n",
    "                compressed_dim=compression_dim,\n",
    "                dropout=dropout\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Shared AggregationEncoder for linking compressed layers inside each iteration\n",
    "        self.aggregation_encoder = AggregationEncoderTransformer(\n",
    "            input_dim=compression_dim,\n",
    "            hidden_dim=agg_hidden_dim,\n",
    "            num_heads=agg_heads,\n",
    "            num_layers=agg_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Final reward predictor processing the sequence of iteration summaries\n",
    "        self.reward_predictor = AttentionToRewardEncoder(\n",
    "            input_dim=agg_hidden_dim,\n",
    "            hidden_dim=reward_hidden_dim,\n",
    "            num_head=reward_heads,\n",
    "            num_layers=reward_layers,\n",
    "            dim_feedforward=reward_ff_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, attention_graph_batches):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        attention_graph_batches : list of lists of lists\n",
    "            [iteration][layer][head] -> each element is a torch_geometric Batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            [batch_size, 1] reward predictions\n",
    "        \"\"\"\n",
    "\n",
    "        iteration_embeddings = []\n",
    "\n",
    "        for iteration_layers in attention_graph_batches:\n",
    "            layer_embeddings = []\n",
    "\n",
    "            for layer_idx, layer_heads in enumerate(iteration_layers):\n",
    "                head_embeddings = []\n",
    "\n",
    "                for head_graph in layer_heads:\n",
    "                    # Shared GNN across all\n",
    "                    gnn_out = self.gnn(head_graph)  # [batch_size, gnn_embedding_dim]\n",
    "                    head_embeddings.append(gnn_out)\n",
    "\n",
    "                # Concat all heads for this layer\n",
    "                head_concat = torch.cat(head_embeddings, dim=-1)  # [batch_size, gnn_embedding_dim * num_heads]\n",
    "                \n",
    "                # Compress layer representation with layer-specific compressor\n",
    "                compressed = self.compressors[layer_idx](head_concat)  # [batch_size, compression_dim]\n",
    "                layer_embeddings.append(compressed.unsqueeze(1))  # keep sequence dimension\n",
    "\n",
    "            # Sequence of compressed layers for this iteration\n",
    "            layer_seq = torch.cat(layer_embeddings, dim=1)  # [batch_size, num_layers, compression_dim]\n",
    "            \n",
    "            # Shared aggregation encoder across iterations\n",
    "            iter_encoded = self.aggregation_encoder(layer_seq)  # [batch_size, num_layers, agg_hidden_dim]\n",
    "\n",
    "            # Pool over layers (mean) to get single iteration summary\n",
    "            iter_summary = iter_encoded.mean(dim=1)  # [batch_size, agg_hidden_dim]\n",
    "            iteration_embeddings.append(iter_summary.unsqueeze(1))\n",
    "\n",
    "        # Stack all iteration summaries into a sequence\n",
    "        iteration_seq = torch.cat(iteration_embeddings, dim=1)  # [batch_size, num_iterations, agg_hidden_dim]\n",
    "\n",
    "        # Final reward prediction\n",
    "        reward = self.reward_predictor(iteration_seq)  # [batch_size, 1]\n",
    "        return reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adverserialAlignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
